[{"pdf:unmappedUnicodeCharsPerPage":["0","0","6","4","0","0","6","0","0","0"],"pdf:PDFVersion":"1.5","xmp:CreatorTool":"LaTeX with hyperref package","pdf:hasXFA":"false","access_permission:modify_annotations":"true","access_permission:can_print_degraded":"true","X-TIKA:Parsed-By-Full-Set":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"X-TIKA:content_handler":"ToXMLContentHandler","pdf:num3DAnnotations":"0","dcterms:created":"2019-12-24T01:38:59Z","dcterms:modified":"2019-12-24T01:38:59Z","dc:format":"application/pdf; version=1.5","pdf:docinfo:creator_tool":"LaTeX with hyperref package","pdf:overallPercentageUnmappedUnicodeChars":"4.556846688501537E-4","access_permission:fill_in_form":"true","pdf:docinfo:modified":"2019-12-24T01:38:59Z","pdf:hasCollection":"false","pdf:encrypted":"false","pdf:docinfo:custom:PTEX.Fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2","pdf:containsNonEmbeddedFont":"true","Content-Length":"1279155","pdf:hasMarkedContent":"false","Content-Type":"application/pdf","PTEX.Fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2","pdf:producer":"pdfTeX-1.40.17","pdf:totalUnmappedUnicodeChars":"16","access_permission:extract_for_accessibility":"true","access_permission:assemble_document":"true","xmpTPg:NPages":"10","resourceName":"e5910c027af0ee9c1901c57f6579d903aedee7f4.pdf","pdf:hasXMP":"false","pdf:charsPerPage":["2923","4836","3742","3532","3231","3504","4103","2190","4726","2325"],"access_permission:extract_content":"true","access_permission:can_print":"true","pdf:docinfo:trapped":"False","X-TIKA:Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"X-TIKA:parse_time_millis":"1259","X-TIKA:embedded_depth":"0","X-TIKA:content":"<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta name=\"pdf:PDFVersion\" content=\"1.5\" />\n<meta name=\"xmp:CreatorTool\" content=\"LaTeX with hyperref package\" />\n<meta name=\"pdf:hasXFA\" content=\"false\" />\n<meta name=\"access_permission:modify_annotations\" content=\"true\" />\n<meta name=\"access_permission:can_print_degraded\" content=\"true\" />\n<meta name=\"dcterms:created\" content=\"2019-12-24T01:38:59Z\" />\n<meta name=\"dcterms:modified\" content=\"2019-12-24T01:38:59Z\" />\n<meta name=\"dc:format\" content=\"application/pdf; version=1.5\" />\n<meta name=\"pdf:docinfo:creator_tool\" content=\"LaTeX with hyperref package\" />\n<meta name=\"access_permission:fill_in_form\" content=\"true\" />\n<meta name=\"pdf:docinfo:modified\" content=\"2019-12-24T01:38:59Z\" />\n<meta name=\"pdf:hasCollection\" content=\"false\" />\n<meta name=\"pdf:encrypted\" content=\"false\" />\n<meta name=\"pdf:docinfo:custom:PTEX.Fullbanner\" content=\"This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2\" />\n<meta name=\"Content-Length\" content=\"1279155\" />\n<meta name=\"pdf:hasMarkedContent\" content=\"false\" />\n<meta name=\"Content-Type\" content=\"application/pdf\" />\n<meta name=\"PTEX.Fullbanner\" content=\"This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2\" />\n<meta name=\"pdf:producer\" content=\"pdfTeX-1.40.17\" />\n<meta name=\"access_permission:extract_for_accessibility\" content=\"true\" />\n<meta name=\"access_permission:assemble_document\" content=\"true\" />\n<meta name=\"xmpTPg:NPages\" content=\"10\" />\n<meta name=\"resourceName\" content=\"e5910c027af0ee9c1901c57f6579d903aedee7f4.pdf\" />\n<meta name=\"pdf:hasXMP\" content=\"false\" />\n<meta name=\"access_permission:extract_content\" content=\"true\" />\n<meta name=\"access_permission:can_print\" content=\"true\" />\n<meta name=\"pdf:docinfo:trapped\" content=\"False\" />\n<meta name=\"X-TIKA:Parsed-By\" content=\"org.apache.tika.parser.DefaultParser\" />\n<meta name=\"X-TIKA:Parsed-By\" content=\"org.apache.tika.parser.pdf.PDFParser\" />\n<meta name=\"access_permission:can_modify\" content=\"true\" />\n<meta name=\"pdf:docinfo:producer\" content=\"pdfTeX-1.40.17\" />\n<meta name=\"pdf:docinfo:created\" content=\"2019-12-24T01:38:59Z\" />\n<title></title>\n</head>\n<body><div class=\"page\"><p />\n<p>Generalizing Deep Models for Overhead Image Segmentation Through\nGetis-Ord Gi* Pooling\n</p>\n<p>Xueqing Deng\nUC Merced\n</p>\n<p>xdeng7@ucmerced.edu\n</p>\n<p>Yi Zhu\nAmazon\n</p>\n<p>yzhu25@ucmerced.edu\n</p>\n<p>Yuxin Tian\nUC Merced\n</p>\n<p>ytian8@ucmerced.edu\n</p>\n<p>Shawn Newsam\nUC Merced\n</p>\n<p>snewsam@ucmerced.edu\n</p>\n<p>Abstract\n</p>\n<p>That most deep learning models are purely data driven\nis both a strength and a weakness. Given sufficient train-\ning data, the optimal model for a particular problem can\nbe learned. However, this is usually not the case and so\ninstead the model is either learned from scratch from a lim-\nited amount of training data or pre-trained on a different\nproblem and then fine-tuned. Both of these situations are\npotentially suboptimal and limit the generalizability of the\nmodel. Inspired by this, we investigate methods to inform\nor guide deep learning models for geospatial image analy-\nsis to increase their performance when a limited amount of\ntraining data is available or when they are applied to sce-\nnarios other than which they were trained on. In particular,\nwe exploit the fact that there are certain fundamental rules\nas to how things are distributed on the surface of the Earth\nand these rules do not vary substantially between locations.\nBased on this, we develop a novel feature pooling method\nfor convolutional neural networks using Getis-Ord G∗\n</p>\n<p>i anal-\nysis from geostatistics. Experimental results show our pro-\nposed pooling function has significantly better generaliza-\ntion performance compared to a standard data-driven ap-\nproach when applied to overhead image segmentation.\n</p>\n<p>1. Introduction\n</p>\n<p>Research in remote sensing has been steadily increasing\nsince it is an important source for Earth observation. Over-\nhead imagery can easily be acquired using low-cost drones\nand no longer requires access to expensive high-resolution\nsatellite or airborne platforms. Since the data provides con-\nvenient and large-scale coverage, people are using it for\na number of societally important problems such as traffic\nmonitoring [21], urban planning [4], vehicle detection [9],\n</p>\n<p>Figure 1: Motivation of our work. The content in the cur-\nrent sliding window is a cluster of pixels of tree. We pro-\npose to incorporate geospatial knowledge to build a pooling\nfunction which can propagate such a spatial cluster during\ntraining, while the standard pooling is not able to achieve it.\n</p>\n<p>land cover segmentation [17], building extraction [36], etc.\nRecently, the analysis of overhead imagery has bene-\n</p>\n<p>fited greatly from deep learning thanks to the significant\nadvancements made by the computer vision community on\nregular (non-overhead) images. However, there still often\nremains challenges when adapting these deep learning tech-\nniques to overhead image analysis, such as the limited avail-\nability of labeled overhead imagery, the difficulty of the\nmodels to generalize between locations, etc.\n</p>\n<p>Annotating overhead imagery is labor intensive so ex-\nisting datasets are often not large enough to train effec-\ntive convolutional neural networks (CNNs) from scratch.\nA common practice therefore is to fine-tune an ImageNet\npre-trained model on a small amount of annotated over-\nhead imagery. However, the generalization capability of\nfine-tuned models is limited as models trained on one lo-\ncation may not work well on others. This is known as the\ncross-location generalization problem and is not necessar-\nily limited to overhead image analysis as it can also be a\n</p>\n<p>1\n</p>\n<p>ar\nX\n</p>\n<p>iv\n:1\n</p>\n<p>91\n2.\n</p>\n<p>10\n66\n</p>\n<p>7v\n1 \n</p>\n<p> [\ncs\n</p>\n<p>.C\nV\n</p>\n<p>] \n 2\n</p>\n<p>3 \nD\n</p>\n<p>ec\n 2\n</p>\n<p>01\n9</p>\n<p />\n</div>\n<div class=\"page\"><p />\n<p>challenge for ground-level imagery such as cross-city road\nscene segmentation [10]. Deep models are often overfit-\nting due to their large capacity yet generalization is partic-\nularly important for overhead images since they can look\nquite different due to variations in the seasons, position of\nthe sun, location variation, etc. For regular image analysis,\ntwo widely adopted approaches to overcome these so-called\ndomain gaps include domain adaptation [12,13,32–34] and\ndata fusion. Both approaches have been adapted by the re-\nmote sensing community [2] to improve performance and\nrobustness.\n</p>\n<p>In this paper, we take a different, novel approach to ad-\ndress the domain gap problem. We exploit the fact that\nthings are not laid out at random on the surface of the Earth\nand that this structure does not vary substantially between\nlocations. In particular, we pose the question of how prior\nknowledge of this structure or, more interestingly, how the\nfundamental rules of geography might be incorporated into\ngeneral CNN frameworks. Inspired by work on physics-\nguided neural networks [15], we develop a framework in\nwhich spatial hotspot analysis informs the feature map pool-\ning. We term this geo-constrained pooling strategy Getis-\nOrd G∗\n</p>\n<p>i pooling and show that it significantly improves the\nsemantic segmentation of overhead imagery particularly in\ncross-location scenarios. To our knowledge, ours is the first\nwork to incorporate geo-spatial knowledge directly into the\nfundamental mechanisms of CNNs. A brief overview of our\nmotivation is shown in Figure 1.\n</p>\n<p>Our contributions are summarized as follows:\n(1) We propose Getis-Ord G∗\n</p>\n<p>i pooling, a novel pooling\nmethod based on spatial Getis-Ord G∗\n</p>\n<p>i analysis of CNN fea-\nture maps. Getis-Ord G∗\n</p>\n<p>i pooling is shown to significantly\nimprove model generalization for overhead image segmen-\ntation.\n</p>\n<p>(2) We establish more generally that using geospatial\nknowledge in the design of CNNs can improve the gener-\nalizability of models which provides the simulated process\nof the data.\n</p>\n<p>2. Related Work\nSemantic segmentation Fully connected neural networks\n(FCN) were recently proposed to improve the semantic seg-\nmentation of non-overhead imagery [20]. Various tech-\nniques have been proposed to boost their performance, such\nas atrous convolution [6–8, 39], skip connections [26], and\npreserving max pooling index for unpooling [3]. And re-\ncently, video is used to scale up training sets by synthesiz-\ning new training samples which is able to improve the accu-\nracy of semantic segmentation networks [41]. Remote sens-\ning research has been driven largely by adapting advances\nin regular image analysis to overhead imagery. In partic-\nular, deep learning approaches to overhead image analy-\nsis have become a standard practice for a variety of tasks,\n</p>\n<p>such as land use/land cover classification [17], building ex-\ntraction [36], road segmentation [23], car detection [9], etc.\nMore literature can be found in a recent survey [40]. And\nvarious segmentation networks have been proposed, such\nrelation-augmentation networks [24] and casnet [19]. How-\never, these methods only adapt deep learning techniques and\nnetworks from regular to overhead images–they do not in-\ncorporate geographic structure or knowledge.\nKnowledge guided neural networks Analyzing over-\nhead imagery is not just a computer vision problem since\nprinciples of the physical world such as geo-spatial rela-\ntionships can help. For example, knowing the road map of a\ncity can definitely improve tasks like building extraction or\nland cover segmentation. While there are no works directly\nrelated to ours, there have been some initial attempts to in-\ncorporate geographic knowledge into deep learning [5, 38].\nChen et al. [5] develop a knowledge-guided golf course\ndetection approach using a CNN fine-tuned on temporally\naugmented data. They also apply area-based rules during\na post-processing step. Zhang et al. [38] propose search-\ning for adjacent parallel line segments as prior spatial infor-\nmation for the fast detection of runways. However, these\nmethods simply fuse prior knowledge from other sources.\nOur proposed method is novel in that we incorporate geo-\nspatial rules into the CNN mechanics. We show later how\nthis helps regularize the model learning and leads to better\ngeneralization.\nPooling functions There are various studies in pooling\nfor image classification as well as segmentation. Lp norm is\nproposed to extend max pooling where intermediate pooling\nfunctions are manually selected between max and average\npooling to better fit the distribution of the input data. [18]\ngeneralizes pooling methods by using a learned linear com-\nbination of max and average pooling. Detail-Preserving\nPooling (DPP) [27] learns weighted summations of pix-\nels over different pooling regions. Salient pixels are more\nimportance in order to achieve higher visual satisfaction.\nStride convolution is used toreplace all max pooling layers\nand activation functions in a small classification model that\nis trained from scratch and achieve better performance [30].\nHowever, stride convolutions are common in segmentation\ntasks. For example, the DeepLab series of networks [7, 8]\nuse stride convolutional layers for feature down-sampling\nrather than max pooling. To enhance detail preservation in\nsegmentation, a recent polynomial pooling approach is pro-\nposed in [35]. However, all these pooling methods are based\non non-spatial statistics. We instead incorporate geo-spatial\nrules/simulation to perform the downsampling.\n</p>\n<p>3. Methods\nIn this section, we investigate how geo-spatial knowl-\n</p>\n<p>edge can be incorporated into standard deep CNNs. We\ndiscuss some general rules from geography to describe geo-</p>\n<p />\n</div>\n<div class=\"page\"><p />\n<p>Figure 2: Given a feature map as an input, max pooling\n(top right) and the proposed G-pooling (bottom right) cre-\nate different output downsampled feature map based on the\ncharacteristics of spatial cluster. The feature map within\nthe sliding window (blue dot line) indicates a spatial clus-\nter. Max pooling takes the max value ignoring the spatial\ncluster, while our G-pooling takes the interpolated value at\nthe center location. (White, gray and black represent three\nvalues range from low to high.)\n</p>\n<p>spatial patterns on the Earth. Then we propose using Getis-\nOrd G∗\n</p>\n<p>i analysis, a common technique for geo-spatial clus-\ntering, to encapsulate these rules. This then informs our\npooling function which is very general and can be used in\nmany network architectures.\n</p>\n<p>3.1. Getis-Ord G∗\ni pooling (G-pooling)\n</p>\n<p>We take inspiration from the well-known first law of ge-\nography: everything is related to everything else, but near\nthings are more related than distant things [31]. While\nthis rule is very general and abstract, it motivates a num-\nber of quantitative frameworks that have been shown to im-\nprove geospatial data analysis. For example, it motivates\nspatial autocorrelation which is the basis for spatial predic-\ntion models like kriging. It also motivates the notion of\nspatial clustering wherein similar things that are spatially\nnearby are more significant than isolated things. Our pro-\nposed framework exploits this to introduce a novel feature\npooling method which we term Getis-Ord G∗\n</p>\n<p>i pooling.\nPooling is used to spatially downsample the feature maps\n</p>\n<p>in deep CNNs. In contrast to standard image downsampling\nmethods which seek to preserve the spatial envelope of pixel\nvalues, pooling selects feature values that are more signif-\nicant in some sense. The most standard pooling method\nis max pooling in which the maximum feature value in a\nwindow is propagated. Other pooling methods have been\nproposed. Average pooling is an obvious choice and is used\n</p>\n<p>in [14,37] for image classification. Strided convolution [16]\nhas also been used. However, max pooling remains by far\nthe most common as it has the intuitive appeal of extract-\ning the maximum activation and thus the most prominent\nfeatures of an image.\n</p>\n<p>However, we postulate that isolated high feature values\nmight not be the most informative and instead develop a\nmethod to propagate clustered values. Specifically, we use\na technique from geostatistics termed hotspot analysis to\nidentify clusters of large values and then propagate a rep-\nresentative from these clusters. Hotspot analysis uses the\nGetis-Ord G∗\n</p>\n<p>i [25] statistic to find locations that have ei-\nther high or low values and are surrounded by locations also\nwith high or low values. These locations are the so-called\nhotspots. The Getis-Ord G∗\n</p>\n<p>i statistic is computed by com-\nparing the local sum of a feature and its neighbors propor-\ntionally to the sum of all features in a spatial region. When\nthe local sum is different from the expected local sum, and\nwhen that difference is too large to be the result of random\nnoise, it will lead to a high positive or low negative G∗\n</p>\n<p>i value\nthat is statistically significant. We focus on locations with\nhigh positive G∗\n</p>\n<p>i values since we want to propagate activa-\ntions.\n</p>\n<p>3.2. Definition\n</p>\n<p>We now describe our G-pooling algorithm in detail.\nPlease see Figure 2 for reference. Similar to other pooling\nmethods, we use a stride sliding window to downsample the\ninput. Given a feature map within the stride window, in or-\nder to compute its G∗\n</p>\n<p>i , we first need to define the weight\nmatrix based on the spatial locations.\n</p>\n<p>We denote the feature values within the sliding window\nas X = x1, x2, ..., xn where n is the number of pixels (lo-\ncations) within the sliding window. We assume the window\nis rectangular and compute the G∗\n</p>\n<p>i statistic at the center of\nthe window. Let the feature value at the center be xi. (If\nthe center does not fall on a pixel location then we compute\nxi as the average of the adjacent values.) The G∗\n</p>\n<p>i statistic\nuses weighed averages where the weights are based on spa-\ntial distances. Let px(xj) and py(xj) denote the x and y\npositions of feature value xj in the image plane. A weight\nmatrix w that measures the Euclidean distance on the image\nplane between xi and the other locations within the sliding\nwindow is then computed as\n</p>\n<p>wi,j =\n√\n</p>\n<p>(px(xi)− px(xj))2 + (py(xi)− py(xj))2. (1)\n</p>\n<p>The Getis-Ord G∗\ni value at location i is now computed as\n</p>\n<p>G∗\ni =\n</p>\n<p>∑n\nj=1 wi,jxj − X̄\n</p>\n<p>∑n\nj=1 wi,j\n</p>\n<p>S\n</p>\n<p>√\n[n\n</p>\n<p>∑n\nj=1 w2\n</p>\n<p>i,j−(\n∑n\n</p>\n<p>j=1 wi,j)2]\n</p>\n<p>n−1\n</p>\n<p>. (2)</p>\n<p />\n</div>\n<div class=\"page\"><p />\n<p>Figure 3: A FCN network architecture with G-pooling.\n</p>\n<p>where X̄ and S are as below,\n</p>\n<p>X̄ =\n</p>\n<p>∑n\nj=1 xj\n</p>\n<p>n\n, (3)\n</p>\n<p>S =\n</p>\n<p>√∑n\nj=1 x\n</p>\n<p>2\nj\n</p>\n<p>n\n− (X̄)2. (4)\n</p>\n<p>Spatial clusters can be detected based on the G∗\ni value.\n</p>\n<p>The higher the value, the more significant the cluster is.\nHowever, the G∗\n</p>\n<p>i value just indicates whether there is a spa-\ntial cluster or not. To achieve our goal of pooling, we need\nto summarize the local region of the feature map by extract-\ning a representative value. We use a threshold to do this.\nIf the computed G∗\n</p>\n<p>i is greater than or equal to the thresh-\nold, a spatial cluster is detected and the value xi is used for\npooling, otherwise the maximum in the window is used.\n</p>\n<p>G − pooling(x) =\n</p>\n<p>{\nxi if G∗\n</p>\n<p>i ≥ threshold\nmax(x) if G∗\n</p>\n<p>i &lt; threshold\n(5)\n</p>\n<p>It’s noted that G∗\ni is in range [-2.8,2.8] where a negative\n</p>\n<p>value indicates a coldspot which means a spatial scatter and\na positive value indicates a hotspot which means a spatial\ncluster. The absolute value |G∗\n</p>\n<p>i | indicates the significance.\nFor example, a high positive G∗\n</p>\n<p>i value indicates the feature\nis more likely to be a spatial cluster.\n</p>\n<p>The output feature map produced by G-pooling is G-\npooling(X) which results after sliding the window over the\nentire input feature map. The threshold is set to 3 different\nvalues in this work, 1.0, 1.5, 2.0. A higher threshold means\nthe current feature map has less chance to be reported as a\nspatial cluster and so max pooling will be applied instead. A\nlower threshold causes more spatial clusters to be detected\nand max pooling will be applied less often. As the threshold\nranges from 1.0 to 1.5 to 2.0, fewer spatial clusters/hotspots\nwill be detected. We find that a threshold of 2.0 results in\nfew hostpots being detected and max pooling mostly to be\nused.\n</p>\n<p>3.3. Network Architecture\n</p>\n<p>A pretrained VGG network [29] is used in our experi-\nments. VGG has been widely used as a backbone in vari-\nous semantic segmentation networks such as FCN [20], U-\nnet [26], and SegNet [3]. In VGG, the standard max pooling\nis a 2×2 window size with a stride of 1. Our proposed G-\npooling uses a 4×4 window size with a stride of 4. There-\nfore, after applying the standard pooling, the size of feature\nmap drops to 1/2, while with our G-pooling it drops to 1/4.\nA small window size is not used in our proposed G-pooling\nsince Getis-Ord G∗\n</p>\n<p>i analysis may not work well in such a\nsmall region. However, we tested the scenario where stan-\ndard pooling is performed with a 4× 4 sliding window and\nthe performance is only slightly different from that using\nthe standard 2 × 2 window. In general, segmentation net-\nworks using VGG16 as the backbone have 5 max pooling\nlayers. So, when we replace max pooling with our pro-\nposed G-pooling, there will be two G-pooling and one max\npooling layers.\n</p>\n<p>4. Experiments\n</p>\n<p>4.1. Dataset\n</p>\n<p>ISPRS dataset We evaluate our method on two image\ndatasets from the ISPRS 2D Semantic Labeling Challenge\n[1]. These datasets are comprised of very high resolution\naerial images over two cities in Germany: Vaihingen and\nPotsdam. While Vaihingen is a relatively small village with\nmany detached buildings and small multi-story buildings,\nPotsdam is a typical historic city with large building blocks,\nnarrow streets and dense settlement structure. The goal is\nto perform semantic labeling of the images using six com-\nmon land cover classes: buildings, impervious surfaces (e.g.\nroads), low vegetation, trees, cars and clutter/background.\nWe report test metrics obtained on the held-out test images.\n</p>\n<p>Vaihingen The Vaihingen dataset has a resolution of 9\ncm/pixel with tiles of approximately 2100 × 2100 pixels.\nThere are 33 images, from which 16 have a public ground\ntruth. Even though the tiles consist of Infrared-Red-Green\n(IRRG) images and DSM data extracted from the Lidar\npoint clouds, we use only the IRRG images in our work.\nWe select five images for validation (IDs: 11, 15, 28, 30 and\n34) and the remaining 11 for training, following [22, 28].\n</p>\n<p>Potsdam The Potsdam dataset has a resolution of 5\ncm/pixel with tiles of 6000 × 6000 pixels. There are 38\nimages, from which 24 have public ground truth. Similar to\nVaihingen, we only use the IRRG images. We select seven\nimages for validation (IDs: 2 11, 2 12, 4 10, 5 11, 6 7, 7 8\nand 7 10) and the remaining 17 for training, again follow-\ning [22, 28].</p>\n<p />\n</div>\n<div class=\"page\"><p />\n<p>Table 1: Experimental results of FCN using VGG-16 as backbone. Stride conv, P-pooling and ours G-pooling are used to\nreplaced the standard max/average pooling.\n</p>\n<p>Potsdam\n</p>\n<p>Methods Roads Buildings Low Veg. Trees Cars mIoU Pixel Acc.\nMax 70.62 74.28 65.94 61.36 61.40 66.72 79.55\nAverage 69.34 74.49 63.94 60.06 60.28 65.62 78.08\nStride 67.22 73.97 63.01 60.09 59.39 64.74 77.54\nP-pooling 71.97 75.55 66.80 62.03 62.39 67.75 81.02\nG-pooling-1.0 (ours) 68.59 77.39 67.48 55.56 62.18 66.24 79.43\nG-pooling-1.5 (ours) 70.06 76.12 67.67 62.12 63.91 67.98 81.63\nG-pooling-2.0 (ours) 70.99 74.89 65.34 61.57 60.77 66.71 79.46\n</p>\n<p>Vaihingen\n</p>\n<p>Max 70.63 80.42 51.57 70.12 55.32 65.61 81.88\nAverage 70.54 79.86 50.49 69.18 54.83 64.98 79.98\nStrde conv 68.36 77.65 49.21 67.34 53.29 63.17 79.44\nP-pooling 71.06 80.52 51.70 70.93 53.65 65.57 82.44\nG-pooling-1.0 (ours) 72.15 79.69 53.28 70.89 53.72 65.95 81.78\nG-pooling-1.5 (ours) 71.61 78.74 48.18 68.53 55.64 64.54 80.42\nG-pooling-2.0 (ours) 71.09 78.88 50.62 68.32 54.01 64.58 80.75\n</p>\n<p>4.2. Experimental settings\n</p>\n<p>Baselines Here, we compare our proposed G-pooling\nwith the standard max-pooling, average-pooling, stride\nconvolution, and the recently proposed P-pooling [35].\nMax/average pooling is commonly for downsampling in the\nsemantic segmentation networks that have VGG as a back-\nbone. ResNet [11] is proposed without using any pooling\nbut strided convolution. Such a network architecture has\nbeen adopted by recent studies for semantic segmentation,\nin particular the DeepLab series [6–8] and PSPNet [39].\nMax pooling is removed and instead strided convolution is\nused to downsample the feature maps while dilated con-\nvolution is used to enlarge the receptive fields. There is\nalso work on detail preserving pooling, for example DDP\n[27] and P-pooling [35]. We select the most recent one,\nP-pooling, which outperforms the other detail preserving\nmethods for comparison.\n</p>\n<p>4.3. Evaluation Metrics\n</p>\n<p>We have two goals in this work, the model’s seg-\nmentation accuracy and its generalization performance.\nModel accuracy is used to report the performance on the\ntest/validation set using the model trained with training set\nwithin one dataset. Model generalizability is used to re-\nport the performance of the test/validation set with another\ndataset. In general, the domain gap between train and\ntest/validation set from one dataset is relatively small. How-\never, cross-dataset testing exists large domain shift problem.\n</p>\n<p>Model accuracy The commonly used per class intersec-\ntion over union (IoU) and mean IoU (mIoU) as well as the\npixel accuracy are adopted for evaluating segmentation ac-\ncuracy.\n</p>\n<p>Model generalizability Specifically, we will perform\nevaluation on the ISPRS Potsdam set with a model trained\non the ISPRS Vahingen set (Potsdam→Vaihingen) and re-\nverse the order (Vaihingen→Potsdam). Pixel accuracy and\nmIoU are used to report the performance of the generaliz-\nability.\n</p>\n<p>4.4. Implementation Details\n</p>\n<p>Implementation of G-pooling Models are implemented\nusing the PyTorch framework. Max-pooling, average-\npooling, stride conv are provided as built-in function and\nP-pooling has open-source code. We implement our G-\npooling in C and use the interface to connect to PyTorch\nfor network training. We adopt the network architecture of\nFCN [20] with a backbone of a pretrained VGG-16 [29].\nThe details of the FCN using our G-pooling can be found in\nSection 3.3. The results in Table 1 are reported using FCN\nwith a VGG-16 backbone.\n</p>\n<p>Training settings Since the image tiles are too large to be\nfed through a deep CNN due to limited GPU memory, we\nrandomly extract image patches of size of 256×256 pixels\nas the training set. Following standard practice, we only use\nhorizontal and vertical flipping as data augmentation during\ntraining. For testing, the whole image is split into 256×256</p>\n<p />\n</div>\n<div class=\"page\"><p />\n<p>Table 2: Cross-location evaluation. We compare the generalization capability of using G-pooling with domain adaptation\nmethod AdaptSegNet which utilize the unlabeled data.\n</p>\n<p>Potsdam→ Vaihingen\n</p>\n<p>Roads Buildings Low Veg. Trees Cars mIoU Pixel Acc.\nMax-pooling 28.75 51.10 13.48 56.00 25.99 35.06 47.48\nstride conv 28.66 50.98 12.76 55.02 24.81 34.45 46.51\nP-pooling 32.87 50.43 13.04 55.41 25.60 35.47 48.94\nOurs (G-pooling) 37.27 54.53 14.85 54.24 27.35 37.65 55.20\nAdaptSegNet 41.54 40.74 21.68 50.45 36.87 38.26 57.73\n</p>\n<p>Vaihingen→ Potsdam\nMax-pooling 20.36 24.51 19.19 9.71 3.65 15.48 45.32\nstride conv 20.65 23.22 16.57 8.73 8.32 15.50 42.28\nP-pooling 23.97 27.66 14.03 10.30 12.07 19.61 44.98\nOurs (G-pooling) 27.05 29.34 33.57 9.12 16.01 23.02 45.54\nAdaptSegNet 40.28 37.97 46.11 15.87 20.16 32.08 50.28\n</p>\n<p>patches with a stride of 256. Then, the predictions of all\npatches are concatenated for evaluation.\n</p>\n<p>We train all our models using Stochastic Gradient De-\nscent (SGD) with an initial learning rate of 0.1, a momen-\ntum of 0.9, a weight decay of 0.0005 and a batch size of 5.\nIf the validation loss plateaus for 3 consecutive epochs, we\ndivide the learning rate by 10. If the validation loss plateaus\nfor 6 consecutive epochs or the learning rate is less than 1e-\n8, we stop the model training. We use a single TITAN V\nGPU for training and testing.\n</p>\n<p>Table 3: The average percentage of detected spatial clusters\nper feature map with different threshold.\n</p>\n<p>Threshold 1.0 1.5 2.0\n</p>\n<p>Potsdam 15.87 9.85 7.65\nVaihingen 14.99 10.44 7.91\n</p>\n<p>5. Effectiveness of G-pooling\n</p>\n<p>In this section, we first show that incorporating geospa-\ntial knowledge into a pooling function of the standard CNN\nlearning can improve segmentation accuracy. Then we\ndemonstrate the promising generalization capability of our\nproposed G-pooling.\n</p>\n<p>The segmentation accuracy on FCN using various pool-\ning functions reported on the test set is shown in Table 1.\nFor G-pooling, we experiment on 3 different thresholds,\nwhich is 1.0, 1.5 and 2.0. The range of G∗\n</p>\n<p>i value is [-2.8,\n2.8]. As explained in Section 3.2, higher G∗\n</p>\n<p>i value can cause\nmore uses of max pooling. If we set the G∗\n</p>\n<p>i value as 2.8,\nthen the case will be all max pooling. Qualitative results\nare shown in Figure 4. And the quantitative results for eval-\n</p>\n<p>uating model accuracy and cross-location generalization is\nshown in Table 1 and 2 respectively.\n</p>\n<p>Non-spatial vs geospatial statistics The baselines of\npooling functions are usually non-spatial statistics, for ex-\nample, finding the max/average value. Our approach pro-\nvides a geospatial process to simulate how things are re-\nlated based on spatial location. Here, we pose the ques-\ntion, “is the knowledge useful to train a deep CNN?”. As\nwe mentioned in Section 3, such a knowledge incorporated\nmethod can bring the benefit of improved generalizability.\nAs shown in Table 1, for Potsdam, using geospatial knowl-\nedge to design the pooling function can bring 1.23% im-\nprovement compared to P-pooling. Our G-pooling-1.0 and\n2.0 is not able to outperform some baselines in the model\naccuracy testing, which indicates the threshold selection is\nimportant. Some classes of the baselines have higher per-\nformance compared to ours. This is expected since the\ndataset is relatively small and may be overfitting. The qual-\nitative results in Figure 4 show our proposed G-pooling has\nless pepper-and-salt effect. In particular, there is less noise\ninside the objects compared to the other methods. This\ndemonstrates our proposed G-pooling simulates the geospa-\ntial distributions and makes the prediction within the objects\nmore compact. The effects of threshold is shown in Table 3,\nas described in Section 3, the higher the threshold the less\nspatial cluster detected.\n</p>\n<p>Domain adaptation vs knowledge incorporation Table\n2 compares using pooling functions with using unsuper-\nvised domain adaptation (UDA). We note that the UDA\nmethod AdaptSegNet [32] uses a large amount of unla-\nbeled data from the target dataset to adapt the model which\nhas been demonstrated to help generalization. The other\nmethods don’t benefit from the unlabeled data. As shown\nin Table 2, our proposed G-pooling is able to achieve the</p>\n<p />\n</div>\n<div class=\"page\"><p />\n<p>best generalization performance. For Potsdam→Vaihingen,\nG-pooling outperforms P-pooling by more than 2%. For\nVaihingen→Potsdam, the improvement is even more sig-\nnificant, at least 3.41%. When we compare the knowledge\nincorporation method G-pooling with the domain adapta-\ntion method AdaptSegNet, the performance difference is\njust 0.61% for Potsdam. The results verify our assump-\ntion that incorporating knowledge helps generalizations as\nwell. And the performance is close to that of domain adap-\ntation which utilizes a great amount of unlabeled data to\nlearn the data distribution. Even though knowledge incor-\nporation doesn’t outperform data-based domain adaptation,\nthese two methods can be combined to provide even better\ngeneralization.\n</p>\n<p>6. G-pooling and state-of-the-art methods\nIn order to verify that our proposed G-pooling is able\n</p>\n<p>to improve state-of-the-art segmentation approaches, we se-\nlect DeepLab [6] and SegNet [3] as additional network ar-\nchitectures to test G-pooling. As mentioned above, the mod-\nels in Section 5 use FCN as the network architecture and\nVGG-16 as the backbone. For fair comparison with FCN,\nVGG-16 is also used as the backbone in DeepLab and Seg-\nNet.\n</p>\n<p>DeepLab [6] uses a large receptive fields through dilated\nconvolution. For the baseline DeepLab itself, pool4 and\npool5 from the backbone VGG-16 are removed and fol-\nlowed by [32] and the dilated conv layers with a dilation rate\nof 2 are replaced with conv5 layers. For the G-pooling ver-\nsion, pool1,pool2 are replaced with G-pooling and we keep\npool3. Thus there are three max pooling layers in the base-\nline and one G-pooling layer and one max pooling layer in\nour proposed version. SegNet uses an encoder-decoder ar-\nchitecture and preserves the max pooling index for unpool-\ning in the decoder. Similar to Deeplab, there are 5 max pool-\ning layers in total in the encoder of SegNet so pool1,pool2\nare replaced with the proposed G pool1 and pool3,pool4 are\nreplaced with G pool2, and pool5 is kept. This leads us to\nuse a 4× 4 unpooling window to recover the spatial resolu-\ntion where the original ones are just 2 × 2. Thus there are\ntwo G-pooling and one max pooling layers in our SegNet\nversion.\n</p>\n<p>As can be seen in Table 4, G-pooling is able to\nimprove the model accuracy for Potsdam, 67.97% →\n68.33%. And the improvement on the generalization test\nPotsdam→Vaihingen is even more obvious, G-pooling im-\nproves mIoU from 38.57 to 40.04. Similar observations can\nbe made for SegNet and FCN. For Vaihingen, even though\nthe model accuracy is not as high as the baseline, the dif-\nference is small. The mIoU of our versions of DeepLab,\nSegNet and FCN is less than 1% lower. We note that Vai-\nhingen is an easier dataset than Potsdam, since it only in-\ncludes urban scenes while Potsdam includes both urban and\n</p>\n<p>nonurban. However, the generalizability of our model using\nG-pooling is much better. As shown, when testing Potsdam\nusing a model trained on Vaihingen, FCN with G-pooling\nis able to achieve 23.02% mIoU which is an improvement\nof 7.54% IoU. The same observations can be made for\nDeepLab and SegNet.\n</p>\n<p>Table 4: Experimental results on comparing w/o and w/ pro-\nposed G-pooling for the state-of-the-art segmentation net-\nworks. P→V indicates the model trained on Potsdam and\ntest on Vaihingen, and versa the verses.\n</p>\n<p>Potsdam (P) P→V\nNetwork G-Pooling mIoU PA mIoU PA\n</p>\n<p>DeepLab\n× 67.97 81.25 38.57 58.47\nX 68.33 80.67 40.04 63.21\n</p>\n<p>SegNet\n× 69.47 82.53 35.98 53.69\nX 70.17 83.27 39.04 56.42\n</p>\n<p>FCN\n× 66.72 79.55 35.06 47.48\nX 67.98 81.63 37.65 55.20\n</p>\n<p>Vaihingen (V) V→P\n</p>\n<p>DeepLab\n× 70.80 83.74 18.44 33.96\nX 70.11 83.09 19.26 36.17\n</p>\n<p>SegNet\n× 66.04 81.79 16.77 45.90\nX 66.71 82.66 25.64 48.08\n</p>\n<p>FCN\n× 65.61 81.88 15.48 45.32\nX 65.95 81.87 23.02 45.54\n</p>\n<p>7. Discussion\n</p>\n<p>Incorporating knowledge is not a novel approach for\nneural networks. Before deep learning, there was work on\nrule-based neural networks which required expert knowl-\nedge to design the network for specific applications. Due\nto the large capacity of deep models, deep learning has\nbecome the primary approach to address vision problems.\nHowever, deep learning is a data-driven approach which\nrelies significantly on the amount of training data. If the\nmodel is trained with a large amount of data then it will\nhave good generalization. But the case is often, particu-\nlarly in overhead image segmentation, that the dataset is not\nlarge enough like it is in ImageNet/Cityscapes. This causes\noverfitting. Early stopping, cross-validation, etc. can help\nto avoid overfitting. Still, if domain shift exists between\nthe training and test sets, the deep models do not perform\nwell. In this work, we propose a knowledge-incorporated\napproach to reduce overfitting. We address the question\nof how to incorporate the knowledge directly into the deep\nmodels by proposing a novel pooling method for overhead\nimage segmentation. But some issues still need discussing\nas follows.</p>\n<p />\n</div>\n<div class=\"page\"><p />\n<p>Figure 4: Qualitative results of ISPRS Potsdam. White: road, blue: building, cyan: low vegetation, green: trees, yellow:\ncars, red: clutter.\n</p>\n<p>Scenarios using G-pooling As mentioned in section 3, G-\npooling is developed using Getis-Ord G∗\n</p>\n<p>i analysis which\nquantifies how the spatial convergence occurs. This is\na simulated process design for geospatial data downsam-\npling. Thus it’s not necessarily appropriate for other image\ndatasets. This is more general restriction of incorporating of\nknowledge. The Getis-Ord G∗\n</p>\n<p>i provides a method to iden-\ntify spatial clusters while training. The effect is similar to\nconditional random fields/Markov random fields in standard\ncomputer vision post-processing methods. However, it is\ndifferent from them since the spatial clustering is dynami-\ncally changing based on the feature maps and the geospatial\nlocation while post-processing methods rely on the predic-\ntion of the models.\n</p>\n<p>Local geospatial pattern We now explain how G-pooling\nworks in deep neural networks. Getis-Ord G∗\n</p>\n<p>i analysis is\nusually used to analyze a global region hotspot detection\nwhich describes the geospatial convergence. As shown in\nFigure 3, G-pooling will be applied twice to downsample\nthe feature map. The spatial size of the G-pooling will be\n64×64 and 16×16 respectively. And the max-pooling will\nlead to the size of feature map being reduced by 1/2 while\nours it will be by 1/4. This is because we want to compute\nG∗\n</p>\n<p>i over a larger region.\n</p>\n<p>Even though G∗\ni is usually computed over a larger re-\n</p>\n<p>gion than in our framework, it still provides captures spatial\nconvergence within a small region. Also, two G-pooling\noperations are applied at different scales of feature map and\nso a larger region in the input image is really considered.\nSpecifically, the first 4 × 4 pooling window is slid over the\n256× 256 feature map and the output feature map has size\n64× 64. This is fed through the next conv layers and a sec-\nond G-pooling is applied. At this stage, the input feature\nmap is 64× 64 and so when a 4× 4 sliding window is now\nused, a region of 16× 16 is really considered, which is 1/16\nof the whole image.\n</p>\n<p>Limitations There are some limitations of our work. For\nexample, we didn’t investigate the optimal window size for\nperforming Getis-Ord G∗\n</p>\n<p>i analysis. We also only consider\none kind of spatial pattern, clusters. And, there might be\nbetter places than pooling to incorporate knowledge in CNN\narchitectures.\n</p>\n<p>8. Conclusion\n</p>\n<p>In this paper, we investigate how geospatial knowledge\ncan be incorporated into deep learning for geospatial im-\nage analysis. We demonstrate that incorporating geospatial</p>\n<p />\n</div>\n<div class=\"page\"><p />\n<p>rules improves performance. We realize, though, that ours\nis just preliminary work into geospatial guided deep learn-\ning. We note the limitations of our approach, for exam-\nple, that the prior distribution does not provide benefits for\nclasses in which this prior knowledge is not relevant. Our\nproposed approach does not show much improvement on\nthe single dataset case especially a small dataset. ISPRS\nVaihingen is a very small dataset which contains around\nonly 500 images of size of 256 × 256. In the future, we\nwill explore other ways to encode geographic rules so they\ncan be incorporated into deep learning models.\n</p>\n<p>References\n[1] ISPRS 2D Semantic Labeling Challenge. http:\n</p>\n<p>//www2.isprs.org/commissions/comm3/wg4/\nsemantic-labeling.html. 4\n</p>\n<p>[2] N. Audebert, B. Saux, and S. Lefvre. Beyond RGB: Very\nHigh Resolution Urban Remote Sensing with Multimodal\nDeep Networks. ISPRS Journal of Photogrammetry and Re-\nmote Sensing, 2018. 2\n</p>\n<p>[3] V. Badrinarayanan, A. Kendall, and R. Cipolla. SegNet: A\nDeep Convolutional Encoder-Decoder Architecture for Im-\nage Segmentation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI), 2017. 2, 4, 7\n</p>\n<p>[4] J. E. Ball, D. T. Anderson, and C. S. Chan. Comprehen-\nsive Survey of Deep Learning in Remote Sensing: Theories,\nTools, and Challenges for the Community. Journal of Ap-\nplied Remote Sensing, 2017. 1\n</p>\n<p>[5] J. Chen, C. Wang, A. Yue, J. Chen, D. He, and X. Zhang.\nKnowledge-guided Golf Course Detection using a Convo-\nlutional Neural Network Fine-tuned on Temporally Aug-\nmented Data. J. Appl. Remote Sens., 2017. 2\n</p>\n<p>[6] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A.\nYuille. DeepLab: Semantic Image Segmentation with Deep\nConvolutional Nets, Atrous Convolution, and Fully Con-\nnected CRFs. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (TPAMI), 2017. 2, 5, 7\n</p>\n<p>[7] L. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethink-\ning Atrous Convolution for Semantic Image Segmentation.\narXiv preprint arXiv:1706.05587, 2017. 2, 5\n</p>\n<p>[8] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam.\nEncoder-Decoder with Atrous Separable Convolution for Se-\nmantic Image Segmentation. In European conference on\ncomputer vision (ECCV), 2018. 2, 5\n</p>\n<p>[9] X. Chen, S. Xiang, C. Liu, and C. Pan. Vehicle Detection in\nSatellite Images by Hybrid Deep Convolutional Neural Net-\nworks. IEEE Geoscience and Remote Sensing Letters, 2014.\n1, 2\n</p>\n<p>[10] Y. Chen, W. Chen, Y. Chen, B. Tsai, Y. Wang, and M. Sun.\nNo More Discrimination: Cross City Adaptation of Road\nScene Segmenters. In International Conference on Com-\nputer Vision (ICCV), 2017. 2\n</p>\n<p>[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2016. 5\n</p>\n<p>[12] J. Hoffman, E. Tzeng, T. Park, J. Zhu, P. Isola, K. Saenko,\nA. Efros, and T. Darrell. CyCADA: Cycle-Consistent Ad-\nversarial Domain Adaptation. In International Conference\non Machine Learning, 2018. 2\n</p>\n<p>[13] J. Hoffman, D. Wang, F. Yu, and T. Darrell. FCNs in the\nWild: Pixel-Level Adversarial and Constraint-based Adapta-\ntion. arXiv preprint arXiv:1612.02649, 2016. 2\n</p>\n<p>[14] G. Huang, Z. Liu, L. Van Der Maaten, and K. Wein-\nberger. Densely Connected Convolutional networks. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2017. 3\n</p>\n<p>[15] A. Karpatne, W. Watkins, J. Read, and V. Kumar. Physics-\nGuided Neural Networks (PGNN): An Application in Lake\nTemperature Modeling. arXiv preprint arXiv:1710.11431,\n2017. 2\n</p>\n<p>[16] J. Kuen, X. Kong, G. Wang, and Y. Tan. DelugeNets: Deep\nNetworks with Efficient and Flexible Cross-Layer Informa-\ntion Inflows. In International Conference on Computer Vi-\nsion (ICCV), 2017. 3\n</p>\n<p>[17] N. Kussul, M. Lavreniuk, S. Skakun, and A. Shelestov. Deep\nLearning Classification of Land Cover and Crop Types Us-\ning Remote Sensing Data. IEEE Geoscience and Remote\nSensing Letters, 2017. 1, 2\n</p>\n<p>[18] C. Lee, P. Gallagher, and Z. Tu. Generalizing pooling func-\ntions in convolutional neural networks: Mixed, gated, and\ntree. In Artificial intelligence and statistics, 2016. 2\n</p>\n<p>[19] Y. Liu, B. Fan, L. Wang, J. Bai, S. Xiang, and C. Pan. Se-\nmantic Labeling in Very High Resolution Images via a Self-\nCascaded Convolutional Neural Network. ISPRS Journal of\nPhotogrammetry and Remote Sensing, 2018. 2\n</p>\n<p>[20] J. Long, E. Shelhamer, and T. Darrell. Fully Convolutional\nNetworks for Semantic Segmentation. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2015.\n2, 4, 5\n</p>\n<p>[21] X. Ma, Z. Dai, Z. He, J. Ma, Y. Wang, and Y. Wang. Learning\nTraffic as Images: A Deep Convolutional Neural Network\nfor Large-Scale Transportation Network Speed Prediction.\nSensors, 2017. 1\n</p>\n<p>[22] E. Maggiori, Y. Tarabalka, G. Charpiat, and P. Alliez. High-\nResolution Aerial Image Labeling with Convolutional Neu-\nral Networks. IEEE Transactions on Geoscience and Remote\nSensing, 2017. 4\n</p>\n<p>[23] V. Mnih and G. E. Hinton. Learning to Detect Roads in High-\nResolution Aerial Images. In European Conference on Com-\nputer Vision (ECCV), 2010. 2\n</p>\n<p>[24] L. Mou, Y. Hua, and X. X. Zhu. A Relation-Augmented\nFully Convolutional Network for Semantic Segmentation in\nAerial Scenes. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019. 2\n</p>\n<p>[25] J. K. Ord and Arthur Getis. Local Spatial Autocorrelation\nStatistics: Distributional Issues and an Application. Geo-\ngraphical Analysis, 1995. 3\n</p>\n<p>[26] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-\nlutional networks for biomedical image segmentation. In\nInternational Conference on Medical image computing and\ncomputer-assisted intervention, 2015. 2, 4</p>\n<p />\n<div class=\"annotation\"><a href=\"http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html\">http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html</a></div>\n<div class=\"annotation\"><a href=\"http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html\">http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html</a></div>\n<div class=\"annotation\"><a href=\"http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html\">http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html</a></div>\n</div>\n<div class=\"page\"><p />\n<p>[27] F. Saeedan, N. Weber, M. Goesele, and S. Roth. Detail-\npreserving pooling in deep networks. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2018.\n2, 5\n</p>\n<p>[28] Jamie Sherrah. Fully Convolutional Networks for Dense Se-\nmantic Labelling of High-Resolution Aerial Imagery. arXiv\npreprint arXiv:1606.02585, 2016. 4\n</p>\n<p>[29] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014. 4, 5\n</p>\n<p>[30] J. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller.\nStriving for simplicity: The all convolutional net. In Inter-\nnational Conference on Learning Representation workshop\n(ICLR workshop), 2015. 2\n</p>\n<p>[31] W. R. Tobler. A Computer Movie Simulating Urban Growth\nin the Detroit Region. Economic Geography, 1970. 3\n</p>\n<p>[32] Y. Tsai, W. Hung, S. Schulter, K. Sohn, M. Yang, and M.\nChandraker. Learning to Adapt Structured Output Space for\nSemantic Segmentation. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2018. 2, 6, 7\n</p>\n<p>[33] Y. Tsai, K. Sohn, S. Schulter, and M. Chandraker. Domain\nAdaptation for Structured Output via Discriminative Repre-\nsentations. In International conference on Computer Vision\n(ICCV), 2019. 2\n</p>\n<p>[34] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial\nDiscriminative Domain Adaptation. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017. 2\n</p>\n<p>[35] Z. Wei, J. Zhang, L. Liu, F. Zhu, F. Shen, Y. Zhou, S. Liu,\nY. Sun, and L. Shao. Building detail-sensitive semantic\nsegmentation networks with polynomial pooling. In IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2019. 2, 5\n</p>\n<p>[36] J. Yuan. Learning Building Extraction in Aerial Scenes\nwith Convolutional Networks. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence (TPAMI), 2017. 1,\n2\n</p>\n<p>[37] S. Zagoruyko and N. Komodakis. Wide Residual Networks.\narXiv preprint arXiv:1605.07146, 2016. 3\n</p>\n<p>[38] P. Zhang, X. Niu, Y. Dou, and F. Xia. Airport Detection\nfrom Remote Sensing Images using Transferable Convolu-\ntional Neural Networks. In International Joint Conference\non Neural Networks (IJCNN), 2016. 2\n</p>\n<p>[39] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid Scene\nParsing Network. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017. 2, 5\n</p>\n<p>[40] X. X. Zhu, D. Tuia, L. Mou, G. Xia, L. Zhang, F. Xu, and\nF. Fraundorfer. Deep Learning in Remote Sensing: A Com-\nprehensive Review and List of Resources. IEEE Geoscience\nand Remote Sensing Magazine, 2017. 2\n</p>\n<p>[41] Y. Zhu, K. Sapra, F. A. Reda, K. J. Shih, S. Newsam, A.\nTao, and B. Catanzaro. Improving semantic segmentation via\nvideo propagation and label relaxation. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2019. 2</p>\n<p />\n</div>\n</body></html>","pdf:annotationTypes":"null","access_permission:can_modify":"true","pdf:docinfo:producer":"pdfTeX-1.40.17","pdf:docinfo:created":"2019-12-24T01:38:59Z","pdf:annotationSubtypes":"Link","pdf:containsDamagedFont":"false"}]